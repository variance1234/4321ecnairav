from __future__ import print_function
import keras
from keras.optimizers import Adam, SGD
from keras.callbacks import ModelCheckpoint, LearningRateScheduler, CSVLogger
from keras.callbacks import ReduceLROnPlateau
from keras.preprocessing.image import ImageDataGenerator
from keras.datasets import cifar100
import numpy as np
import os
import scipy.ndimage

from wideresnet_original import WideResNet28_10

import keras.backend as K

import functools


def lr_schedule(epoch):
    """Learning Rate Schedule
    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.
    Called automatically every epoch as part of callbacks during training.
    # Arguments
        epoch (int): The number of epochs
    # Returns
        lr (float32): learning rate
    """
    lr = 0.1
    # lr = 0.001
    if epoch > 160:
        lr *= 0.2
    if epoch > 120:
        lr *= 0.2
    if epoch > 60:
        lr *= 0.2
    print('Learning rate: ', lr)
    return lr


def train(model_path):
    # Training parameters
    batch_size = 128
    epochs = 200
    num_classes = 100

    # Load the CIFAR10 data.
    (x_train, y_train), (x_test, y_test) = cifar100.load_data()

    # Normalize data.
    x_train = x_train.astype('float32') / 255.0
    x_test = x_test.astype('float32') / 255.0

    # Subtract pixel mean and scale to std
    # x_train = x_train.astype('float32')
    # x_train = (x_train - x_train.mean(axis=0)) / (x_train.std(axis=0))
    # x_test = x_test.astype('float32')
    # x_test = (x_test - x_test.mean(axis=0)) / (x_test.std(axis=0))

    x_mean = np.array([125.3, 123.0, 113.9]) / 255.0
    x_std = np.array([63.0, 62.1, 66.7]) / 255.0

    for i in range(3):
        x_train[:, :, :, i] -= x_mean[i]
        x_train[:, :, :, i] /= x_std[i]
        x_test[:, :, :, i] -= x_mean[i]
        x_test[:, :, :, i] /= x_std[i]

    print('x_train shape:', x_train.shape)
    print(x_train.shape[0], 'train samples')
    print(x_test.shape[0], 'test samples')
    print('y_train shape:', y_train.shape)

    # Convert class vectors to binary class matrices.
    y_train = keras.utils.to_categorical(y_train, num_classes)
    y_test = keras.utils.to_categorical(y_test, num_classes)

    # model = WideResNet28_10(weights=None)
    model = WideResNet28_10()
    model_type = 'wideresnet28-10'

    # optimizer=Adam(lr=lr_schedule(0)
    # optimizer = Adam()

    optimizer = SGD(momentum=0.9, lr=lr_schedule(0))

    model.compile(loss='categorical_crossentropy',
                  optimizer=optimizer,
                  metrics=['acc'])

    model.summary()
    print(model_type)

    # Prepare model model saving directory.
    save_dir = os.path.join(os.getcwd(), 'saved_models')
    model_name = 'cifar100_%s_model.{epoch:03d}.h5' % model_type
    if not os.path.isdir(save_dir):
        os.makedirs(save_dir)
    filepath = os.path.join(save_dir, model_name)

    lr_scheduler = LearningRateScheduler(lr_schedule)

    # Prepare callbacks for model saving and for learning rate adjustment.
    checkpoint = ModelCheckpoint(filepath=filepath,
                                 monitor='val_acc',
                                 verbose=1,
                                 save_best_only=True)

    csv_logger = CSVLogger('wideresnet_training_performance.csv', append=True)

    callbacks = [lr_scheduler, checkpoint, csv_logger]
    # callbacks = [checkpoint, csv_logger]

    # Run training, with or without data augmentation.
    datagen = ImageDataGenerator(width_shift_range=0.125,
                                 height_shift_range=0.125,
                                 fill_mode="reflect",
                                 horizontal_flip=True)

    # datagen = ImageDataGenerator()

    # Compute quantities required for featurewise normalization
    # (std, mean, and principal components if ZCA whitening is applied).
    datagen.fit(x_train)

    # Get loss and accuracy value before the first epoch
    val_loss, val_acc = model.evaluate(x_test, y_test, batch_size=128)
    train_loss, train_acc = model.evaluate(x_train, y_train, batch_size=128)

    p_out_f = open("wideresnet_training_performance.csv", "w")
    p_out_f.write('epoch,acc,loss,val_acc,val_loss\n')
    p_out_f.write('%d, %.5f, %.5f, %.5f, %.5f\n' % (-1, train_acc, train_loss, val_acc, val_loss))
    p_out_f.close()

    # Fit the model on the batches generated by datagen.flow().
    model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),
                        validation_data=(x_test, y_test),
                        epochs=epochs, verbose=1, workers=4,
                        callbacks=callbacks)

    # Score trained model.
    scores = model.evaluate(x_test, y_test, verbose=1)
    print('Test loss:', scores[0])
    print('Test accuracy:', scores[1])

    model.save(model_path)
    print('Finished training model and saved model at:', model_path)


def main():
    train(model_path='./WideResNet28-10.h5')


if __name__ == "__main__":
    main()
